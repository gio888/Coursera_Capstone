{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction/Business Problem\n",
    "The COVID-19 pandemic has taken an unprecedented toll globally, already affecting over 2M people, taking over 200K lives and, according to WEF, will likely cost the world 2 trillion USD in economic losses.\n",
    "\n",
    "Not all cities are affected by the virus the same way. We want to see how poverty incidence, a metric used to measured the level of poverty in a geographic area, is a predictor for severity of problem as measured by cases per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Section\n",
    "For this exercise, we will be using the following datasets:\n",
    "\n",
    "1. **Case Information dataset**\n",
    "    \n",
    "    **Source:** Philippine Department of Health. See https://www.doh.gov.ph/2019-nCoV\n",
    "    \n",
    "    **Description:** List of confirmed COVID-19 cases from the DOH Epidemiological Bureau.\n",
    "\n",
    "    **Dataset fields:**\n",
    "\t* CaseCode : Random code assigned for labelling cases\n",
    "\t* Age : Age\n",
    "\t* AgeGroup : Five-year age group\n",
    "\t* Sex : Sex\n",
    "\t* DateRepConf : Date publicly announced as confirmed case\n",
    "\t* DateRecover : Date recovered\n",
    "\t* DateDied : Date died\n",
    "\t* RemovalType : Type of removal (recovery or death)\n",
    "\t* DateRepRem : Date publicly announced as removed\n",
    "\t* Admitted : Binary variable indicating patient has been admitted to hospital\n",
    "\t* RegionRes : Region of residence\n",
    "\t* ProvCityRes : Province of residence\n",
    "\t* RegionPSGC : Philippine Standard Geographic Code of Region of Residence\n",
    "\t* ProvPSGC : Philippine Standard Geographic Code of Province of Residence\n",
    "\t* MunCityPSGC : Philippine Standard Geographic Code of Municipality or City of Residence\n",
    "\t* HealthStatus : Known current health status of patient (asymptomatic, mild, severe, critical, died, recovered)\n",
    "\t* Quarantined : Ever been home quarantined, not necessarily currently in home quarantine\n",
    "\n",
    "\n",
    "2. **Municipal and City Level Poverty Estimates**\n",
    "\n",
    "    **Source:** The Philippine Statistics Authority (PSA). See https://psa.gov.ph/content/psa-releases-2015-municipal-and-city-level-poverty-estimates\n",
    "\n",
    "    **Description:** This is a set of estimates using the small area estimation (SAE) technique. See https://psa.gov.ph/sites/default/files/Technical%20Notes%20on%202015%20SAE.pdf\n",
    "\n",
    "    **Dataset fields:**\n",
    "    * PSGC\n",
    "    * Region/Province\n",
    "    * Municipality/City\n",
    "    * Poverty Incidence\n",
    "\n",
    "\n",
    "2. **Philippine Standard Geographic Code (PSGC)**\n",
    "\n",
    "    **Source:** The Philippine Statistics Authority (PSA). See https://psa.gov.ph/content/psa-releases-2015-municipal-and-city-level-poverty-estimates\n",
    "\n",
    "    **Description:** The PSGC is a systematic classification and coding of geographic areas in the Philippines. It is based on the four (4) well-established hierarchical levels of geographical-political subdivisions of the country, namely, the administrative region, the province, the municipality/city, and the barangay.\n",
    "\n",
    "    **Dataset fields:**\n",
    "    \n",
    "    * Column \"Code\": PSGC Code\n",
    "    * Column \"Income Classification\"\n",
    "    * Column \"Class\" (Average Annual Income)\n",
    "        * Provinces:\n",
    "            * 1st\tP 450M or more\n",
    "            * 2nd\tP 360M or more but less than P 450M\n",
    "            * 3rd\tP 270M or more but less than P 360M\n",
    "            * 4th\tP 180M or more but less than P 270M\n",
    "            * 5th\tP 90M or more but less than P 180M\n",
    "            * 6th\tBelow P 90M\n",
    "        * Cities\n",
    "            * 1st\tP 400M or more\n",
    "            * 2nd\tP 320M or more but less than P 400M\n",
    "            * 3rd\tP 240M or more but less than P 320M\n",
    "            * 4th\tP 160M or more but less than P 240M\n",
    "            * 5th\tP 80M or more but less than P 160M\n",
    "            * 6th\tBelow P 80M\n",
    "        * Municipalities\n",
    "            * 1st\tP 55M or more\n",
    "            * 2nd\tP 45M or more but less than P 55M\n",
    "            * 3rd\tP 35M or more but less than P 45M\n",
    "            * 4th\tP 25M or more but less than P 35M\n",
    "            * 5th\tP 15M or more but less than P 25M\n",
    "            * 6th\tBelow P 15M\t\t\n",
    "    * Column \"Urban / Rural\"\n",
    "\t* Column \"Population\"\t\n",
    "    \n",
    "\n",
    "\n",
    "2. Geojson dataset of cities and municipalities in the Philippines. We will use this dataset to provide the mapping boundaries of the different cities and municipalities in the Philippines.\n",
    "\t* ID_0 : Unique ID 0\n",
    "\t* ISO : ISO Country Code\n",
    "\t* NAME_0 : Country Name\n",
    "\t* NAME_2 : Municipality or City Name\n",
    "\t* PROVINCE : Province Name\n",
    "\t* REGION : Regiona Name\n",
    "\t* geometry : Polygon, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "section which represents the main component of the report where you discuss and describe any exploratory data analysis that you did, any inferential statistical testing that you performed, if any, and what machine learnings were used and why.\n",
    "\n",
    "We will use the CRISP-DM (Cross-Industry Process for Data Mining) methodology. The CRISP-DM methodology is well-proven methodology in data science. CRISP-DM loosely and iteratively follows six major phases:\n",
    "\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "As we have have covered #1 and #2 previously, we will continue with Data Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We start by importing all necessary libraries and installing all dependencies for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan\n",
    "\n",
    "- [Setup](#setup)\n",
    "    - import libraries\n",
    "- [Prepare data](#data_prep)\n",
    "    - DOH data\n",
    "        - explore\n",
    "        - clean\n",
    "        - select\n",
    "    - Poverty data\n",
    "        - explore\n",
    "        - clean\n",
    "        - select\n",
    "    - City index data\n",
    "        - explore\n",
    "        - clean\n",
    "        - select\n",
    "    - City Latlong\n",
    "        - load json\n",
    "        - explore\n",
    "        - clean\n",
    "        - select\n",
    "    - Merge data\n",
    "        - calculate case/pop as predicted variable\n",
    "        - calculate distance from manila as additional feature\n",
    "- Predict\n",
    "- Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"import_lib\"></a>\n",
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json5 as json # library to handle JSON files\n",
    "import requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium # map rendering library\n",
    "import seaborn as sns\n",
    "import xlrd\n",
    "#import shapely\n",
    "#import fiona\n",
    "#import pyproj\n",
    "import geopandas as gpd\n",
    "\n",
    "#prep\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#validation libraries\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='data_prep'></a>\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOH Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We define the columns and their proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define column types\n",
    "col_types = {'Age' : 'float64', \n",
    "             'AgeGroup' : 'category', \n",
    "             'Sex' : 'category', \n",
    "             'RemovalType' : 'category', \n",
    "             'Admitted' : 'category', \n",
    "             'RegionRes' : 'category', \n",
    "             'ProvRes' : 'category', \n",
    "             'CityMunRes' : 'category', \n",
    "             'CityMuniPSGC' : 'category',\n",
    "             'BarangayRes' : 'category',\n",
    "             'BarangayPSGC' : 'category',\n",
    "             'HealthStatus' : 'category', \n",
    "             'Quarantined' : 'category', \n",
    "             'Pregnanttab' : 'category', \n",
    "             'ValidationStatus' : 'category'}\n",
    "col_date = [4, 5, 6, 7, 8, 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We read the csv and load it into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_raw = pd.read_csv(\"DOH COVID Data Drop_ 20201013 - 04 Case Information.csv\", dtype = col_types, parse_dates = col_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploratory Data Analysis\n",
    "1. We do a quick check on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check our dataframe\n",
    "df_cases_raw.info()\n",
    "df_cases_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine unique values per column\n",
    "df_cases_raw.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Since our study wants to see if certain city statistic predicts cases, we will limit our variables to cases and cities, i.e CityMuniPSGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = df_cases_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = df_cases[['CaseCode','DateRepConf','CityMuniPSGC','CityMunRes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We drop the rows where we can't identify the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Nan cities\n",
    "df_cases = df_cases.loc[-df_cases.CityMuniPSGC.isna(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For consistency, we rename CityMuniPSGC column to PSGC, which stands for Philippine Standard Geographic Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = df_cases.rename(columns={\"CityMuniPSGC\": \"PSGC\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Then we group the dataframe into cities are represented by PSGC and count the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_by_city = df_cases.groupby(['PSGC','CityMunRes'])['CaseCode'].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Then we transform PSGC to standard nine-digit code for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_by_city.PSGC = df_cases_by_city.PSGC.str.replace('PH','')\n",
    "df_cases_by_city.rename(columns={\"CaseCode\": \"Cases\"}, inplace = True)\n",
    "df_cases_by_city.rename(columns={\"CityMunRes\": \"Name\"}, inplace = True)\n",
    "df_cases_by_city.sort_values('PSGC', inplace = True)\n",
    "df_cases_by_city.set_index('PSGC', inplace = True)\n",
    "#check\n",
    "df_cases_by_city.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. To limit our dataset, we remove the cities with no cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_by_city = df_cases_by_city.loc[df_cases_by_city.Cases > 0,:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Convert city names to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_by_city.Name = df_cases_by_city.Name.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Quick checking if data makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_by_city.sort_values(by = 'Cases', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_by_city.loc[df_cases_by_city.Name.str.contains('df_cases_by_city')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_by_city.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poverty Incidence dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We read the poverty incidence data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read city stats\n",
    "df_poverty = pd.read_csv('City and Municipal-level Small Area Poverty Estimates_ 2009, 2012 and 2015_0.csv',\n",
    "                  skiprows = [0,1,2,3,4],\n",
    "                  encoding = \"ISO-8859-1\",\n",
    "                  names = ['PSGC_ID','Poverty Incidence'],\n",
    "                  usecols = [0,5],\n",
    "                  dtype = {'PSGC_ID':'object','Poverty Incidence':np.float16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We clean it up by dropping NaNs and invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NaN indexes\n",
    "df_poverty = df_poverty.loc[df_poverty.index.dropna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop other invalid rows\n",
    "df_poverty = df_poverty.loc[-df_poverty.PSGC_ID.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For consistency on merge, we transform the PSGC codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poverty['PSGC'] = df_poverty.PSGC_ID.str.zfill(6)\n",
    "df_poverty['PSGC'] = df_poverty.PSGC.str.ljust(9, '0')\n",
    "df_poverty.sort_values('PSGC', inplace = True)\n",
    "df_poverty.drop('PSGC_ID', axis =1, inplace = True)\n",
    "df_poverty.set_index('PSGC', inplace = True)\n",
    "df_poverty.sort_index(inplace = True)\n",
    "#check\n",
    "df_poverty.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poverty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poverty.loc['133902000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City index data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We load city data containing geographic code and other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex = pd.read_excel('PSGC 2Q 2020 Publication.xlsx',\n",
    "                    sheet_name = 'PSGC',\n",
    "                    usecols = [i for i in range(7)],\n",
    "                    names = ['PSGC',\n",
    "                             'Name',\n",
    "                             'Geographic Level',\n",
    "                             'City Class',\n",
    "                             'Income Classification',\n",
    "                             'Urban Rural',\n",
    "                             'Population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We filter the dataset to only include cities, municipalities and sub municipalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex = df_cityindex[df_cityindex['Geographic Level'].isin(['City','Mun','SubMun'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For our purposes, we replace NaNs with 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex = df_cityindex.fillna('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We examine each feature to see if we can further clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex['Geographic Level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex['City Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex['Income Classification'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex['Income Classification'] = df_cityindex['Income Classification'].str.replace(r'\\*.*','')\n",
    "df_cityindex['Income Classification'] = df_cityindex['Income Classification'].str.replace(r' .*','')\n",
    "df_cityindex['Income Classification'] = df_cityindex['Income Classification'].str.replace('-','UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex['Urban Rural'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Urban Rural only contains UNK, we drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.drop('Urban Rural', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. As before, we transform the geographic code for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.PSGC = df_cityindex.PSGC.astype('str')\n",
    "df_cityindex.PSGC = df_cityindex.PSGC.str.zfill(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Then we sort the dateframe according the the geographic code and make that the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.sort_values('PSGC', inplace = True)\n",
    "df_cityindex.set_index('PSGC', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For consistency, we transform city name to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.Name = df_cityindex.Name.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Then we change the features into category type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex = df_cityindex.astype(dtype = {'Name':'category',\n",
    "                                            'Geographic Level':'category',\n",
    "                                            'City Class':'category',\n",
    "                                            'Income Classification':'category'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Quick checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cityindex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.loc['133902000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities Geodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We load the geojson data on Philippine cities and municipalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_geodata = gpd.read_file('/Users/gio/Google Drive/Education/IBM Data Science Professional Certificate/Capstone/Municities') \n",
    "df_geodata = gpd.read_file('https://github.com/altcoder/philippines-psgc-shapefiles/raw/master/source/2015/Municities.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Other possible geojson files\n",
    "\n",
    "'https://github.com/faeldon/philippines-json-maps/tree/master/geojson/municties/medres'\n",
    "'https://github.com/justinelliotmeyers/official_philippines_shapefile_data_2016'\n",
    "'https://raw.githubusercontent.com/macoymejia/geojsonph/master/MuniCities/MuniCities.json'\n",
    "'https://github.com/altcoder/philippines-psgc-shapefiles/blob/master/source/2015/Municities.zip'\n",
    "'https://raw.githubusercontent.com/macoymejia/geojsonph/master/MuniCities/MuniCities.minimal.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As we are interested in finding out whether distance from the capital is a factor, we need to calculate the distance from Manila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geodata['Centroid'] = df_geodata.geometry.centroid\n",
    "manila_loc = gpd.tools.geocode('Manila')\n",
    "df_geodata['Distance from Manila'] = gpd.GeoSeries(df_geodata.Centroid).distance(manila_loc.iloc[0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. And as usual, let's do a clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other invalid rows\n",
    "df_geodata = df_geodata.loc[-df_geodata['Distance from Manila'].isna(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geodata['PSGC'] = df_geodata.ADM3_PCODE.str.replace('PH','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change index to PSGC\n",
    "df_geodata.set_index('PSGC', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Quick checking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geodata.loc['133902000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. We merge the datasets into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_geodata, df_cityindex, how = 'inner', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_merged, df_cases_by_city, how = 'inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_merged, df_poverty, how = 'inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can add 'Density' as additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Density'] = df_merged.Population/df_merged.Shape_Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We clean up irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(['Name_x','Name_y', 'Shape_Leng', 'date', 'validOn', 'validTo', \n",
    "                'ADM3_PCODE', 'ADM3_REF', 'ADM3ALT1EN', 'ADM3ALT2EN', 'ADM2_EN', 'ADM2_PCODE', 'ADM1_EN', \n",
    "                'ADM1_PCODE', 'ADM0_EN', 'ADM0_PCODE', 'Shape_Area'], \n",
    "               axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.rename(columns = {'ADM3_EN' : 'Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We now define 'Case Per Capita\" as our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Case Per Capita'] = df_merged.Cases/df_merged.Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "So now we have a dataset of cities and municipalities with feature statistics like city class, income classification, population, poverty incidences, distance from manila, and density as possible predictors of cases per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We check our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.sort_values('Case Per Capita', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's get rid of rows with NaN Density values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.dropna(axis=0, how='any', thresh=None, subset=['Density'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Since we have explicitly changed NaNs to literal UNK categorical values, it would be worth checking how many UNKs are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change UNK for different columns\n",
    "for columns in df_merged.columns:\n",
    "    print(f'Percent UNK Column {columns} = {((sum(df_merged[columns] == \"UNK\"))/len(df_merged[columns])):.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This points to feature City Class to have mostly UNKs so let's get that off our list of features. So we get rid of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(['City Class'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We will now run some quick correlation analysis to determine which of the features could predict our target variable. Note that we exclude Cases and Population as they are already captured in Case Per Capita. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rearrange columns for easier reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[['Name', 'geometry', 'Centroid', 'Geographic Level', 'Income Classification', \n",
    "                       'Population', 'Cases', 'Distance from Manila', 'Poverty Incidence', 'Density', \n",
    "                       'Case Per Capita']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_merged, vars = ['Distance from Manila','Poverty Incidence', 'Density', 'Case Per Capita'])\n",
    "g.fig.suptitle(\"Pair Plot\", y=1.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_merged.corr().iloc[2:,2:],annot=True,lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore conclude that of the continous variables we have, Density has the strongest correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Separating the analysis by geographic level shows correlation to be stronger at City level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now explore our data if we group them according to Geographic Level and Income Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do a quick pairplot correlation analysis while differentiating the colors by Geographic Level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_merged, hue = \"Geographic Level\", vars = ['Distance from Manila','Poverty Incidence', 'Density', 'Case Per Capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_levels = df_merged['Geographic Level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in geo_levels:\n",
    "    sub_df = df_merged.loc[df_merged['Geographic Level'] == level,['Distance from Manila','Poverty Incidence', \n",
    "                                                                   'Density', 'Case Per Capita']]\n",
    "    print(level)\n",
    "    #print('\\n')\n",
    "    print(sub_df.corr())\n",
    "    print('\\n\\n')\n",
    "    g = sns.pairplot(sub_df)\n",
    "    g.fig.suptitle(level, y=1.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with our earlier observation, we can see that even when we drilled down to Geographic Level, Density still has the strongest correlation. Interestingly, at the Sub Municipality Level, the correlation is still strong but negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the effects if we group them by Income Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_merged, hue = \"Income Classification\", vars = ['Distance from Manila','Poverty Incidence', 'Density', 'Case Per Capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_classes = df_merged['Income Classification'].unique()\n",
    "\n",
    "for level in income_classes:\n",
    "    sub_df = df_merged.loc[df_merged['Income Classification'] == level,['Distance from Manila','Poverty Incidence','Density','Case Per Capita']]\n",
    "    print(level)\n",
    "    #print('\\n')\n",
    "    print(sub_df.corr())\n",
    "    print('\\n\\n')\n",
    "    g = sns.pairplot(sub_df)\n",
    "    g.fig.suptitle(level, y=1.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When drilling down Income Class, our earlier observation remains consistent, with almost perfect correlation in 1st Class cities. The correlation weakens after the 4th Class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metro Manila Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most the of cases occur in Metro Manila, let us see how our variables predict Cases Per Capita in the Metro Manila Area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that cities in the National Capital Region of Metro Manila has a PSGC of 130000000 or more. Limiting our dataset to Metro Manila cities we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psgc_mm = ['133900000', '137401000', '137402000', '137403000', '137404000', '137405000', '137501000', '137502000', '137503000', '137504000', '137601000', '137602000', '137603000', '137604000', '137606000', '137605000', '137607000']\n",
    "psgc_mm = ['137401000', '137402000', '137403000', '137404000', '137405000', '137501000', '137502000', '137503000', '137504000', '137601000', '137602000', '137603000', '137604000', '137606000', '137605000', '137607000']\n",
    "df_mm = df_merged.loc[psgc_mm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_mm, vars = ['Distance from Manila','Poverty Incidence', 'Density', 'Case Per Capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_mm.corr().iloc[2:,2:],annot=True,lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, for Metro Manila, Density is not such a strong predictor. Interestingly, Distance from the center of old Manila, is a negative strong predictor. At least in the Metro capital area, the farther you are from old manila, the safer it is, from a Cases Per Capita perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['Distance from Manila','Poverty Incidence', 'Density']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize variables\n",
    "\n",
    "#initialize scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "data = df2[['Distance from Manila','Poverty Incidence', 'Density']]\n",
    "mm.fit(data)\n",
    "df2[['Distance from Manila scaled','Poverty Incidence scaled', 'Density scaled']] = list(mm.transform(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[['Name', 'geometry', 'Centroid', 'Distance from Manila','Geographic Level', 'Income Classification', \n",
    "     'Population', 'Cases','Poverty Incidence', 'Density', 'Distance from Manila scaled', \n",
    "     'Poverty Incidence scaled','Density scaled', 'Case Per Capita']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df2, vars = ['Distance from Manila scaled','Poverty Incidence scaled', 'Density scaled', 'Case Per Capita'])\n",
    "g.fig.suptitle(\"Pair Plot\", y=1.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df2.corr().iloc[5:,5:],annot=True,lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Separating the analysis by geographic level shows correlation to be stronger at City level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now explore our data if we group them according to Geographic Level and Income Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do a quick pairplot correlation analysis while differentiating the colors by Geographic Level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2, hue = \"Geographic Level\", vars = ['Distance from Manila','Poverty Incidence', 'Density', 'Case Per Capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_levels = df2['Geographic Level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in geo_levels:\n",
    "    sub_df = df2.loc[df2['Geographic Level'] == level,['Distance from Manila','Poverty Incidence', \n",
    "                                                                   'Density', 'Case Per Capita']]\n",
    "    print(level)\n",
    "    #print('\\n')\n",
    "    print(sub_df.corr())\n",
    "    print('\\n\\n')\n",
    "    g = sns.pairplot(sub_df)\n",
    "    g.fig.suptitle(level, y=1.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with our earlier observation, we can see that even when we drilled down to Geographic Level, Density still has the strongest correlation. Interestingly, at the Sub Municipality Level, the correlation is still strong but negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the effects if we group them by Income Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2, hue = \"Income Classification\", vars = ['Distance from Manila','Poverty Incidence', 'Density', 'Case Per Capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_classes = df2['Income Classification'].unique()\n",
    "\n",
    "for level in income_classes:\n",
    "    sub_df = df2.loc[df2['Income Classification'] == level,['Distance from Manila','Poverty Incidence','Density','Case Per Capita']]\n",
    "    print(level)\n",
    "    #print('\\n')\n",
    "    print(sub_df.corr())\n",
    "    print('\\n\\n')\n",
    "    g = sns.pairplot(sub_df)\n",
    "    g.fig.suptitle(level, y=1.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When drilling down Income Class, our earlier observation remains consistent, with almost perfect correlation in 1st Class cities. The correlation weakens after the 4th Class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metro Manila Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most the of cases occur in Metro Manila, let us see how our variables predict Cases Per Capita in the Metro Manila Area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that cities in the National Capital Region of Metro Manila has a PSGC of 130000000 or more. Limiting our dataset to Metro Manila cities we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psgc_mm = ['133900000', '137401000', '137402000', '137403000', '137404000', '137405000', '137501000', '137502000', '137503000', '137504000', '137601000', '137602000', '137603000', '137604000', '137606000', '137605000', '137607000']\n",
    "psgc_mm = ['137401000', '137402000', '137403000', '137404000', '137405000', '137501000', '137502000', '137503000', '137504000', '137601000', '137602000', '137603000', '137604000', '137606000', '137605000', '137607000']\n",
    "df_mm = df2.loc[psgc_mm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mm = df_mm[['Name', 'geometry', 'Centroid', 'Distance from Manila','Geographic Level', 'Income Classification', \n",
    "               'Population', 'Cases','Poverty Incidence', 'Density', 'Distance from Manila scaled', \n",
    "               'Poverty Incidence scaled','Density scaled', 'Case Per Capita']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_mm, vars = ['Distance from Manila','Poverty Incidence', 'Density', 'Case Per Capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_mm.corr().iloc[5:,5:],annot=True,lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mm.plot(column='Case Per Capita', cmap='OrRd', scheme='quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map()\n",
    "m.choropleth(df_mm, data = df_mm, columns=['Name', 'Case Per Capita'], fill_color='YlOrBr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "* mapped visualizations\n",
    "* get rid of non normalized data\n",
    "* write report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy variables\n",
    "\n",
    "#change UNK for different columns\n",
    "for columns in df_merged.columns:\n",
    "    print(f'Number of Unknowns for Column {columns} = {sum(df_merged[columns] == \"UNK\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.get_dummies(df_merged, \n",
    "               columns = ['Geographic Level', 'Income Classification'], \n",
    "               prefix = ['GL','IC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[['Name', 'Population', 'Poverty Incidence', 'GL_City', \n",
    "     'GL_Mun', 'GL_SubMun', 'IC_1st', 'IC_2nd','IC_3rd', 'IC_4th', \n",
    "     'IC_5th', 'IC_6th', 'IC_Special', 'IC_UNK','Case Per Capita']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check histogram of population\n",
    "df_merged['Population'][df_merged['IC_3rd'] == 1].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize population\\\n",
    "ss = StandardScaler()\n",
    "df_merged['Population Normalized'] = ss.fit_transform(df_merged[['Population']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.corr().loc['Case Per Capita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target variable\n",
    "y = df_merged['Case Per Capita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_merged[['Population Normalized', 'Poverty Incidence', 'GL_City', 'GL_Mun', 'GL_SubMun', 'IC_1st', 'IC_2nd','IC_3rd', 'IC_4th', 'IC_5th', 'IC_6th', 'IC_Special', 'IC_UNK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and test sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2)\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a linear model\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X_valid)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdgCV = RidgeCV(alphas=[0.01,0.1,1,10,100,1000], cv=5)\n",
    "rdgCV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdgCV.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdg = Ridge(alpha=10)\n",
    "rdg.fit(X_train, y_train)\n",
    "rdg.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rdg.predict(X_valid)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1, n_estimators=100)\n",
    "rfr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.score(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfr.predict(X_valid)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.coef_)\n",
    "print(np.argmax(lm.coef_))\n",
    "print(df_merged.columns[np.argmax(lm.coef_)])\n",
    "print(rdgCV.coef_)\n",
    "print(np.argmax(rdgCV.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(X_train,y_train)\n",
    "\n",
    "y_lm_pred = lm.predict(X_train)\n",
    "y_rdgCV_pred = rdgCV.predict(X_train)\n",
    "y_rfr_pred = rfr.predict(X_train)\n",
    "\n",
    "print('-----training score ---')\n",
    "print(lm.score(X_train, y_train))\n",
    "print(rdgCV.score(X_train, y_train))\n",
    "print(rfr.score(X_train, y_train))\n",
    "print('----Validation score ---')\n",
    "print(lm.score(X_valid, y_valid))\n",
    "print(rdgCV.score(X_valid, y_valid))\n",
    "print(rfr.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_merged.corr().loc['Case Per Capita'], robust = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "section where you discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "section where you discuss any observations you noted and any recommendations you can make based on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "section where you conclude the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras below\n",
    "## Next Steps\n",
    "* merge geojson in main dataframe\n",
    "* calculate distance from manila\n",
    "* determine correlation between distance from manila\n",
    "* do same analysis as earlier with baranggay level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_remove = 'Geographic Level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_new = list(sub_df_cases_raw.columns)\n",
    "col_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_merged[['Geographic Level','Population','Poverty Incidence','Case Per Capita']], hue = 'Geographic Level',\n",
    "            diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_by_level(dataframe, variable):\n",
    "    for level in geo_levels:\n",
    "        sub_df = df_merged.loc[df_merged['Geographic Level'] == level,['Population', 'Poverty Incidence','Case Per Capita']]\n",
    "    print(level)\n",
    "    print('\\n')\n",
    "    #print(sub_df_cases_raw.corr().loc['Case Per Capita'])\n",
    "    sns.pairplot(sub_df)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in geo_levels:\n",
    "    sub_df = df_merged.loc[df_merged['Geographic Level'] == level,['Population', 'Poverty Incidence','Case Per Capita']]\n",
    "    print(level)\n",
    "    print('\\n')\n",
    "    #print(sub_df_cases_raw.corr().loc['Case Per Capita'])\n",
    "    sns.pairplot(sub_df)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check use of \"city of\" or \"<city name> city\"\n",
    "df_cityindex.loc[df_cityindex.Name.str.contains('city'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex['Name2'] = df_cityindex.Name.str.replace('city of','')\n",
    "df_cityindex['Name2'] = df_cityindex.Name2.str.replace(r'\\(capital\\)','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cityindex.loc[df_cityindex.Name.str.contains('city'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Geographic Level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[df_merged['Geographic Level'] == 'Mun',['Population', 'Poverty Incidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in df_merged['Geographic Level'].unique():\n",
    "    sub_df = df_merged.loc[df_merged['Geographic Level'] == level,['Population', 'Poverty Incidence','Case Per Capita']]\n",
    "    print(level)\n",
    "    print('\\n')\n",
    "    #print(sub_df_cases_raw.corr().loc['Case Per Capita'])\n",
    "    sns.pairplot(sub_df)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Income Classification'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = gpd.read_file('https://raw.githubusercontent.com/macoymejia/geojsonph/master/MuniCities/MuniCities.minimal.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Centroid'] = df1.geometry.centroid\n",
    "manila_loc = gpd.tools.geocode('Manila')\n",
    "df1['Distance from Manila'] = gpd.GeoSeries(df1.Centroid).distance(manila_loc.iloc[0,0])\n",
    "\n",
    "#drop other invalid rows\n",
    "df1 = df1.loc[-df1.Distance from Manila.isna(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.NAME_2 = df1.NAME_2.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1.NAME_2.str.contains('city'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['NAME_3'] = df1.NAME_2.str.replace(' city','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up NaNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.sort_values(by = 'Distance from Manila')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_geodata = gpd.read_file('https://github.com/altcoder/philippines-psgc-shapefiles/blob/master/source/2015/Municities.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_geodata = pd.read_csv('https://github.com/altcoder/philippines-psgc-shapefiles/blob/master/datasets/CSV/Municipalities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "    \n",
    "* bug reported in stackoverflow https://stackoverflow.com/questions/65229994/geopandas-readfile-not-recognizing-a-legit-shape-file and github of repository https://github.com/faeldon/philippines-json-maps/issues/1.\n",
    "\n",
    "* follow up with faeldon bot@renovateapp.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
